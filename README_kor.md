````markdown
<!-- ì–¸ì–´ ìŠ¤ìœ„ì²˜ -->
<p align="right">
  <a href="README.md">
    <img alt="English" src="https://img.shields.io/badge/EN-English-blue?style=flat-square">
  </a>
  <img alt="í•œêµ­ì–´" src="https://img.shields.io/badge/KR-Korean-black?style=flat-square">
</p>

# AIâ€‘Architectureâ€‘Trendâ€‘Analysis

[![PyPI - Python](https://img.shields.io/badge/python-v3.10%2B-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/WoosopYi/AI-Architecture-Trend-Analysis?style=social)](https://github.com/WoosopYi/AI-Architecture-Trend-Analysis/stargazers)

<img src="docs/assets/pipeline_diagram.png" width="38%" align="right" alt="Pipeline overview" />

**AIâ€‘Architectureâ€‘Trendâ€‘Analysis**ëŠ” ìº¡ì…˜ í˜•íƒœì˜ í…ìŠ¤íŠ¸ë¡œë¶€í„° ê±´ì¶• ë””ìì¸ íŠ¸ë Œë“œë¥¼ **ì¶”ì¶œÂ·í´ëŸ¬ìŠ¤í„°ë§Â·í•´ì„**í•˜ëŠ” ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

- **íŒŒì´í”„ë¼ì¸:** Sentenceâ€‘Transformers â†’ UMAP/HDBSCAN (BERTopic) â†’ LLM ê¸°ë°˜ ì¬ë¶„ë¥˜  
- **ì…ë ¥:** ì´ë¯¸ì§€ ìº¡ì…˜ ë˜ëŠ” ì •ë¦¬ëœ ì„¤ëª… *(ìº¡ì…˜ì€ VLMìœ¼ë¡œ ë³„ë„ ìƒì„± ê°€ëŠ¥í•˜ë©°, ë³¸ ë…¸íŠ¸ë¶ì€ í…ìŠ¤íŠ¸ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤)*

---

## âœ¨ ì£¼ìš” íŠ¹ì§•

- **í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì‹œì‘:** ìº¡ì…˜/ì„¤ëª… í…ìŠ¤íŠ¸ë§Œ ìˆìœ¼ë©´ ë¼ë²¨ë§ ì—†ì´ ë°”ë¡œ ë¶„ì„.
- **BERTopic ê¸°ë°˜ ë°œê²¬:** ì„ë² ë”© â†’ UMAP â†’ HDBSCAN â†’ câ€‘TFâ€‘IDFë¡œ ëª…í™•í•˜ê³  í•´ì„ ê°€ëŠ¥í•œ í† í”½ ë„ì¶œ.
- **ë„ë©”ì¸ ì¹œí™” ì¬ë¶„ë¥˜:** LLM(ë˜ëŠ” ê·œì¹™/í”„ë¡¬í”„íŠ¸)ë¡œ ì›ì‹œ í† í”½ì„ ê±´ì¶•ê°€ ì¹œí™” ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘.
- **í”„ë¡¬í”„íŠ¸ ìƒì„±:** ì¹´í…Œê³ ë¦¬ í•µì‹¬ í‚¤ì›Œë“œë¡œ ìƒì„±í˜• ì´ë¯¸ì§€â€‘AIìš© í”„ë¡¬í”„íŠ¸ ìë™ êµ¬ì„±.
- **ë‹¨ì¼ ë…¸íŠ¸ë¶ ì›Œí¬í”Œë¡œ:** `AI_Trend_Analysis.ipynb` í•˜ë‚˜ë¡œ ì¬í˜„ì„± ìˆê²Œ ì „ ê³¼ì •ì„ ì‹¤í–‰.

---

## ğŸ“¦ ë¦¬í¬ì§€í† ë¦¬ êµ¬ì„±

ë¦¬í¬ì§€í† ë¦¬ í•œëˆˆì— ë³´ê¸°:

| ê²½ë¡œ | ìš©ë„ |
|---|---|
| `AI_Trend_Analysis.ipynb` | ì—”ë“œâ€‘íˆ¬â€‘ì—”ë“œ ë©”ì¸ ë…¸íŠ¸ë¶ |
| `Data/` | ì…ë ¥ CSV ì €ì¥ ìœ„ì¹˜(ì˜ˆ: `data.csv`) |
| `docs/assets/` | ë‹¤ì´ì–´ê·¸ë¨/ì´ë¯¸ì§€ ìì‚°(ì˜ˆ: `pipeline_diagram.png`) |
| `output/` | ë…¸íŠ¸ë¶ ì¶œë ¥ë¬¼(CSV/JSON/í”Œë¡¯) |
| `prompt/` | JSON ì „ì²˜ë¦¬ìš© LLM í”„ë¡¬í”„íŠ¸ |
| `requirements1.txt` | íŒŒì´ì¬ ì˜ì¡´ì„± ëª©ë¡ |
| `LICENSE` | MIT ë¼ì´ì„ ìŠ¤ |
| `README.md` Â· `README.kor.md` | ë¬¸ì„œ |

<details>
<summary><strong>í´ë” íŠ¸ë¦¬</strong> (í¼ì¹˜ê¸°)</summary>

```text
AI-Architecture-Trend-Analysis/
â”œâ”€â”€ AI_Trend_Analysis.ipynb
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ data.csv                
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ assets/
â”‚       â””â”€â”€ pipeline_diagram.png
â”œâ”€â”€ output/
â”œâ”€â”€ prompt/
â”œâ”€â”€ requirements1.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
````

</details>

---

##  ë¹ ë¥¸ ì‹œì‘

í´ë¡  â†’ ì„¤ì¹˜ â†’ ë°ì´í„° ì¤€ë¹„ â†’ (ì„ íƒ) ë¡œì»¬ GGUF ëª¨ë¸ ì¶”ê°€ â†’ ì‹¤í–‰:

```bash
git clone https://github.com/WoosopYi/AI-Architecture-Trend-Analysis.git
cd AI-Architecture-Trend-Analysis

# 1) ì˜ì¡´ì„± ì„¤ì¹˜ (Python 3.10+ ê¶Œì¥)
pip install -r requirements1.txt

# requirements íŒŒì¼ì´ ì—†ê±°ë‚˜ ë…¸íŠ¸ë¶ ë‚´ ì„¤ì¹˜ì™€ ë™ì¼í•˜ê²Œ í•˜ë ¤ë©´:
pip install --upgrade pip plotly kaleido
pip install "llama-cpp-python" bertopic datasets jinja2 spacy spacy-transformers
python -m spacy download en_core_web_trf

# 2) ì…ë ¥ CSVë¥¼ ./Data/data.csv ë¡œ ì¤€ë¹„ (ì•„ë˜ ìŠ¤í‚¤ë§ˆ ì°¸ê³ )

# 3) (ì„ íƒ) ë¡œì»¬ LLM í† í”½ ë¼ë²¨ë§ìš© GGUF ëª¨ë¸ì„ ./model/ ì— ë°°ì¹˜
# ì˜ˆ: openhermes-2.5-mistral-7b.Q4_K_M.gguf (quantized)
# mkdir -p model && cd model
# wget https://huggingface.co/.../openhermes-2.5-mistral-7b.Q4_K_M.gguf

# 4) ë…¸íŠ¸ë¶ì„ ìœ„ì—ì„œ ì•„ë˜ë¡œ ì‹¤í–‰
jupyter lab AI_Trend_Analysis.ipynb
```

**í•˜ë“œì›¨ì–´ íŒ**

* `BAAI/bge-m3` ì„ë² ë”© ë° `llama-cpp-python` ê°€ì†ì„ ìœ„í•´ **GPU ê¶Œì¥** (`n_gpu_layers=-1` ì„¤ì •).
* **CPUâ€‘only**ë„ ë™ì‘í•˜ì§€ë§Œ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

##  ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ (CSV)

`./Data/data.csv`ì— ìµœì†Œ **`description`** ì»¬ëŸ¼(ì´ë¯¸ì§€/í”„ë¡œì íŠ¸ë³„ ìº¡ì…˜í˜• í…ìŠ¤íŠ¸)ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.

```csv
description
"Slender concrete structure with vertical fins and recessed glazing..."
"Timber lattice facade with perforated metal screens..."
"Terraced green roof massing with porous podium..."
```

> ë…¸íŠ¸ë¶ì€ `load_dataset("csv", data_files="Data/data.csv", encoding="cp949")["train"]`ë¡œ ë¡œë“œí•˜ê³  `dataset["description"]`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. UTFâ€‘8 íŒŒì¼ì´ë¼ë©´ `encoding="utf-8-sig"`ë¡œ ë³€ê²½í•˜ì„¸ìš”.

---

##  Stepâ€‘byâ€‘Step

ì•„ë˜ ë‹¨ê³„ëŠ” **ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ ê²½ë¡œ/í•¨ìˆ˜/íŒŒë¼ë¯¸í„°**ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ê° ë‹¨ê³„ë§ˆë‹¤ **í•µì‹¬ ìš”ì•½**ì„ ë§ë¶™ì˜€ìŠµë‹ˆë‹¤.

### 0) í™˜ê²½ ì„¤ì •(í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜)

ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ì¡´ì„±ì„ í•œ ë²ˆ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
!pip install --upgrade plotly kaleido
!pip install --upgrade pip

# Install llama-cpp-python (CMake options can be adjusted)
...
!pip install jinja2 # For token-level distribution visualization
!pip install spacy spacy-transformers en-core-web-trf
!python -m spacy download en_core_web_trf

# (Optional) GPU acceleration stacks
!pip install cudf-cu12 dask-cudf-cu12 --extra-index-url=https://pypi.nvidia.com
!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com
!pip install cugraph-cu12 --extra-index-url=https://pypi.nvidia.com
!pip install cupy-cuda12x -f https://pip.cupy.dev/aarch64

# (Optional) Extra visualization library
!git clone https://github.com/TutteInstitute/datamapplot.git
!pip install datamapplot/.
```

---

### 0â€‘B) (ì„ íƒ) ì–‘ìí™” LLM ë‹¤ìš´ë¡œë“œ

ë¡œì»¬ GGUF ëª¨ë¸ì„ ë°›ì•„ `/model`ì— ë‘ê³  ì˜¤í”„ë¼ì¸/ê³ ì† ë¼ë²¨ë§ì— ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
#!wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf
```

ì €ì¥ ê²½ë¡œ: `/app/paper/model/openhermes-2.5-mistral-7b.Q4_K_M.gguf`

---

### 1) ì„í¬íŠ¸ & spaCy íŒŒì´í”„ë¼ì¸ ë¡œë”©

ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ì— ì‚¬ìš©í•  `en_core_web_trf`ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

```python
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import spacy
nlp = spacy.load("en_core_web_trf")

import numpy as np
import pandas as pd
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

import plotly
import plotly.express as px

from datasets import load_dataset
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN

# BERTopic related
from bertopic.representation import KeyBERTInspired, LlamaCPP
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
```

---

### 2) ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €(spaCy ê¸°ë°˜)

ë¶ˆìš©ì–´/ë„ë©”ì¸ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•´ í† í”½ì˜ í•´ì„ë ¥ì„ ë†’ì…ë‹ˆë‹¤.

```python
domain_stopwords = {
    "set", "none", "nothing", "unidentified", "not", "unvisible",
    "there", "was", "is", "in", "of", "feature", "building", "fa?ade",
    "structure", "context", "element", "architectural", "emphasize",
    "visual", "create", "clearly", "include", "overhead", "provide",
    "compose", "highlight", "dramatic", "area", "earth", "form", "mass",
    "perspective", "style", "scale", "absence", "depth", "proportion",
    # ... (list continues in the notebook)
}

def spacy_tokenizer(doc_text: str):
    doc = nlp(doc_text)
    tokens = []
    for token in doc:
        lemma = token.lemma_.lower().strip()

        # 1) remove basic stopwords/punct/whitespace/very short tokens
        if token.is_stop or token.is_punct or lemma == "" or len(lemma) < 2:
            continue

        # 2) remove domain stopwords
        if lemma in domain_stopwords:
            continue

        tokens.append(lemma)
    return tokens
```

---

### 3) CSV ë¡œë“œ

`description` ì»¬ëŸ¼ì„ ê°–ëŠ” ìº¡ì…˜í˜• í…ìŠ¤íŠ¸ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.

```python
from datasets import load_dataset

dataset = load_dataset(
    "csv",
    data_files="/app/paper/data/data.csv",
    encoding='cp949'  # or 'utf-8-sig'
)["train"]

docs = dataset["description"]

print("Sample document count::", len(docs))
print("First document example:", docs[0])
```

---

### 4) ë¡œì»¬ Llama ëª¨ë¸ ë¡œë“œ(`llama_cpp_python`)

BERTopicì˜ `LlamaCPP` í‘œí˜„ ëª¨ë¸ë¡œ ì‚¬ìš©í•  GGUF LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

```python
llm = Llama(
    model_path="/app/paper/model/openhermes-2.5-mistral-7b.Q4_K_M.gguf",
    n_gpu_layers=-1,
    n_ctx=4000,
    stop=["Q:", "\n"]
)

label_prompt = """Q:

You are an expert in architecture.

I have a topic that contains the following documents:

[DOCUMENTS]

The topic is described by the following keywords: '[KEYWORDS]'.

Based on the above information, can you give a short label of the topic of at most 5 words? 
Focus on architectural keywords

A:
"""

representation_model = {
    "KeyBERT": KeyBERTInspired(),
    "LLM": LlamaCPP(llm, prompt=label_prompt),
}
```

---

### 5) ì„ë² ë”© ëª¨ë¸ & ë²¡í„°ë¼ì´ì €

`BAAI/bge-m3`ë¡œ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³ , spaCy ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¥¼ ë²¡í„°ë¼ì´ì €ì— ì ìš©í•©ë‹ˆë‹¤.

```python
# (1) SentenceTransformer embedding
embedding_model = SentenceTransformer("BAAI/bge-m3", device="cuda")
embeddings = embedding_model.encode(docs, show_progress_bar=True)

# (2) Custom CountVectorizer (spaCy)
custom_vectorizer = CountVectorizer(
    tokenizer=spacy_tokenizer,
    token_pattern=None,
    # (optional) min_df, max_df, ngram_range, ...
)
```

---

### 6) UMAP/HDBSCAN(í´ëŸ¬ìŠ¤í„°ë§)

ì°¨ì› ì¶•ì†Œ í›„ ë°€ì§‘ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì•„ ì£¼ì œêµ°ì„ í˜•ì„±í•©ë‹ˆë‹¤.

```python
# UMAP/HDBSCAN used for actual clustering
umap_model = UMAP(
    n_neighbors=6,
    n_components=4,
    min_dist=1,
    spread=1.5,
    metric='cosine',
    random_state=32
)

hdbscan_model = HDBSCAN(
    min_cluster_size=5,
    metric='euclidean',
    cluster_selection_method='eom',
    prediction_data=True
)

# 2-D UMAP for visualization
reduced_embeddings_2d = UMAP(
    n_neighbors=5,
    n_components=2,
    min_dist=1,
    spread=1.5,
    metric='cosine',
    random_state=32
).fit_transform(embeddings)
```

---

### 7) BERTopic ìƒì„± & í•™ìŠµ

ì„ë² ë”©/ì°¨ì›ì¶•ì†Œ/í´ëŸ¬ìŠ¤í„°ë§/í‘œí˜„ ëª¨ë¸ì„ í•˜ë‚˜ì˜ BERTopic íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.

```python
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    representation_model=representation_model,
    top_n_words=30,
    calculate_probabilities=True,
    verbose=True,
    vectorizer_model=custom_vectorizer
)

topics, probs = topic_model.fit_transform(docs, embeddings)
topic_info = topic_model.get_topic_info()
print("Generated top 30 topic info:")
print(topic_info.head(30))
```

**í† í”½ í…Œì´ë¸” ë‚´ë³´ë‚´ê¸°:**

```python
info = topic_model.get_topic_info()
info.to_csv("/app/Github/output/topic_model.csv", index=True)
```

---

### 8) ì‹œê°í™”(ê°œìš” â†’ ê³„ì¸µ â†’ ë¶„í¬)

ë‚´ì¥ ì¸í„°ë™í‹°ë¸Œ í”Œë¡¯ìœ¼ë¡œ í† í”½/ë¬¸ì„œ/ê³„ì¸µ êµ¬ì¡°ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.

```python
# (1) Topics
fig_topics = topic_model.visualize_topics(); fig_topics.show()

# (2) Documents in 2D
fig_docs_2d = topic_model.visualize_documents(
    docs, reduced_embeddings=reduced_embeddings_2d, hide_document_hover=False
); fig_docs_2d.show()

# (3) Hierarchical topics (tree)
hierarchical_topics = topic_model.hierarchical_topics(docs)
fig_hierarchy = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
fig_hierarchy.show()

# (4) Hierarchical documents + topics
fig_h_docs_2d = topic_model.visualize_hierarchical_documents(
    docs, hierarchical_topics, reduced_embeddings=reduced_embeddings_2d, hide_document_hover=False
); fig_h_docs_2d.show()

# (5) Barchart of topic terms
fig_barchart = topic_model.visualize_barchart(top_n_topics=24, n_words=10); fig_barchart.show()

# (6) Topic similarity heatmap
fig_heatmap = topic_model.visualize_heatmap(n_clusters=10); fig_heatmap.show()

# (7) Term rank / score decline
fig_term_rank = topic_model.visualize_term_rank(); fig_term_rank.show()

# (8) Probability distribution for one document
fig_dist = topic_model.visualize_distribution(probs[0], min_probability=0.0); fig_dist.show()
```

---

### 9) í† í”½ë³„ í‚¤ì›Œë“œ í…Œì´ë¸” êµ¬ì¶•

`(topic_id, keyword, score)`ë¥¼ ì¶”ì¶œÂ·ì •ë ¬í•´ LLM ì¬ë¶„ë¥˜ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
topic_keywords = []
all_topics = topic_model.get_topic_info().Topic.unique()
for t_id in all_topics:
    if t_id == -1:
        continue
    top_words = topic_model.get_topic(t_id)  # [(word, score), ...]
    for (w, score) in top_words:
        topic_keywords.append({
            "topic_id": int(t_id),
            "keyword": w,
            "score": float(score),
        })

df_keywords = pd.DataFrame(topic_keywords)
df_keywords.sort_values(["topic_id","score"], ascending=[True,False], inplace=True)
```

**LLM ì…ë ¥ JSON ë‚´ë³´ë‚´ê¸°:**

```python
import json

items_for_llm = []
for _, row in df_keywords.iterrows():
    items_for_llm.append({
        "topic_id": int(row["topic_id"]),
        "keyword": row["keyword"],
    })

with open("/app/Github/output/topic_keywords_for_llm.json", "w", encoding="utf-8") as f:
    json.dump(items_for_llm, f, ensure_ascii=False, indent=2)
```

---

### 10) LLMìœ¼ë¡œ í‚¤ì›Œë“œ ì „ì²˜ë¦¬(ë…¸íŠ¸ë¶ ì™¸ë¶€)

ê° `(topic_id, keyword)`ë¥¼ **ê±´ì¶• ì¹´í…Œê³ ë¦¬**ë¡œ ë§¤í•‘í•´ ì €ì¥í•©ë‹ˆë‹¤.

* í”„ë¡¬í”„íŠ¸ íŒŒì¼: `'/Github/Prompt/json_pre_process_LLM_prompt.txt'`
* ê²°ê³¼ ì €ì¥: `"/app/Github/output/architecture_category_llm_output.json"`

ì˜ˆì‹œ JSON:

```json
[
  {"topic_id":0,"keyword":"window","category":"Facade composition"},
  {"topic_id":0,"keyword":"concrete","category":"Materials and Textures"}
]
```

---

### 11) LLM ì¹´í…Œê³ ë¦¬ ë³‘í•©

ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬ë¥¼ í‚¤ì›Œë“œ í…Œì´ë¸”ì— ì¡°ì¸í•˜ê³  `"Unknown"`ì„ ì œê±°í•©ë‹ˆë‹¤.

```python
import json
import pandas as pd

with open("/app/Github/output/architecture_category_llm_output.json", "r", encoding="utf-8") as f:
    data = json.load(f)

df_classified = pd.DataFrame(data)

df_merged = pd.merge(
    df_keywords,
    df_classified,
    on=["topic_id","keyword"],
    how="left"
)

# Remove items with "Unknown"
df_merged = df_merged[df_merged["category"] != "Unknown"]
print(df_merged.head(210))
```

---

### 12) ì¹´í…Œê³ ë¦¬ ì§‘ê³„: ì ìˆ˜ & ë¹ˆë„

ì¹´í…Œê³ ë¦¬ë³„ \*\*ë¹ˆë„(í‚¤ì›Œë“œ ìˆ˜)\*\*ì™€ \*\*ì¤‘ìš”ë„(ì ìˆ˜ í•©)\*\*ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

```python
# Sum of scores per category
df_cat_scores = df_merged.groupby("category")["score"].sum().reset_index()
df_cat_scores.sort_values("score", ascending=False, inplace=True)
print(df_cat_scores)

# Count of keywords per category
cat_counts = df_merged.groupby("category")["keyword"].count().reset_index()
cat_counts = cat_counts.sort_values("keyword", ascending=False)
print("\n=== Number of keywords per category ===")
print(cat_counts)

# (Optional) another alias in the notebook
df_scoresum = df_merged.groupby("category")["score"].sum().reset_index()
df_scoresum = df_scoresum.sort_values("score", ascending=False)
print("\n=== Sum of cTFIDF per category ===")
print(df_scoresum)
```

---

### 13) ì¹´í…Œê³ ë¦¬ ë¶„í¬ ì‹œê°í™”

**ë¹ˆë„**ì™€ **ì ìˆ˜ í•©** ê¸°ì¤€ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

```python
import plotly.express as px

cat_scores = df_merged.groupby("category")["score"].sum().reset_index(name="total_score")

fig1 = px.bar(cat_counts.sort_values("keyword", ascending=False), x="category", y="keyword",
              title="Category distribution (by keyword frequency)")
fig1.show()

fig2 = px.bar(cat_scores.sort_values("total_score", ascending=False), x="category", y="total_score",
              title="Category importance (sum of câ€‘TFâ€‘IDF scores)")
fig2.show()

TOP_N = 5
top_categories_count = cat_counts.head(TOP_N)
print(f"\n=== Top {TOP_N} categories based on total keyword appearances ===")
print(top_categories_count)

TOP_N_SCORE = 5
cat_scores_sorted = cat_scores.sort_values("total_score", ascending=False)
top_categories_score = cat_scores_sorted.head(TOP_N_SCORE)
print(f"\n=== Top {TOP_N_SCORE} categories based on total keyword scores ===")
print(top_categories_score)
```

---

### 14) ì¹´í…Œê³ ë¦¬ë³„ Topâ€‘N í‚¤ì›Œë“œ(ì ìˆ˜ ê¸°ì¤€)

ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì ìˆ˜ í•© ê¸°ì¤€ ìƒìœ„ í‚¤ì›Œë“œë¥¼ ë‚˜ì—´í•˜ê³ , í•„ìš” ì‹œ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

```python
import pandas as pd
import plotly.express as px
from IPython.display import display

# ===== User settings =====
KEYWORDS_PER_CATEGORY = 10
SHOW_BAR_CHARTS = True
# =========================

# 1) Keyword-level aggregation: freq (count), total_score (sum)
agg = (df_merged.groupby(["category", "keyword"])
       .agg(freq=("keyword", "count"), total_score=("score", "sum"))
       .reset_index())

# 2) For each category, get top-N by total_score
for cat in sorted(agg["category"].unique()):
    sub = agg[agg["category"] == cat].copy()
    top_n = sub.sort_values("total_score", ascending=False).head(KEYWORDS_PER_CATEGORY)

    print(f"\n=== Category: {cat} ===")
    display(top_n)

    if SHOW_BAR_CHARTS and not top_n.empty:
        fig = px.bar(top_n, x='keyword', y='total_score', text='freq',
                     title=f"Top {KEYWORDS_PER_CATEGORY} keywords in '{cat}' (by score)")
        fig.show()
```

---

### 15) ìƒì„±í˜• ì´ë¯¸ì§€â€‘AIìš© í”„ë¡¬í”„íŠ¸ ìƒì„±

ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ í‚¤ì›Œë“œë¡œ ì§§ì€ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤(ì ìˆ˜/ë¹ˆë„ ê¸°ì¤€ ì„ íƒ).

```python
def make_image_prompt(category_name, df_merged, top_k=5, by_score=True):
    """
    Select top_k keywords by score (or frequency) in a category and compose an example prompt.
    """
    subset = df_merged[df_merged["category"] == category_name]
    if subset.empty:
        return None

    if by_score:
        key_sorted = (subset.groupby("keyword")["score"]
                      .sum().reset_index(name="total_score")
                      .sort_values("total_score", ascending=False))
        top_k_words = key_sorted.head(top_k)["keyword"].tolist()
    else:
        freq_sorted = (subset.groupby("keyword")["score"]
                       .count().reset_index(name="freq")
                       .sort_values("freq", ascending=False))
        top_k_words = freq_sorted.head(top_k)["keyword"].tolist()

    if len(top_k_words) == 0:
        return None

    return (f"A futuristic '{category_name}' architecture design focusing on "
            f"{', '.join(top_k_words)}. Ultra-detailed, photorealistic rendering.")

# Example usage (use your top categories list)
for cat_name in cat_counts.head(5)["category"]:
    prompt = make_image_prompt(cat_name, df_merged, top_k=5, by_score=True)
    print(f" - [{cat_name}] Prompt: {prompt}" if prompt else f" - [{cat_name}] -> No keyword in the category")
```

---

### 16) LLM ê¸°ë°˜ ìµœì¢… í•´ì„

ì¬ë¶„ë¥˜ ê²°ê³¼(í‚¤ì›Œë“œ, ë¹ˆë„, ì ìˆ˜)ë¥¼ ë°”íƒ•ìœ¼ë¡œ `final_analysis.txt` í”„ë¡¬í”„íŠ¸ ë“±ìœ¼ë¡œ **ì„œìˆ í˜• ì¸ì‚¬ì´íŠ¸**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

> ë…¸íŠ¸ë¶ ë§ˆì§€ë§‰ ë§ˆí¬ë‹¤ìš´ ì°¸ê³ : *â€œUse LLM and reâ€‘categorized results (keyword, frequency, scores) for Final Analysis by LLM (use `final_analysis.txt` prompt)â€*.

---

##  ë³€ê²½ ê°€ëŠ¥í•œ ì„¤ì •

* **CSV ê²½ë¡œ/ì¸ì½”ë”©:** `Data/data.csv`, `encoding="cp949"` â†’ íŒŒì¼ í™˜ê²½ì— ë§ì¶° ë³€ê²½
* **spaCy ëª¨ë¸:** `en_core_web_trf`(ëŒ€í˜• íŠ¸ëœìŠ¤í¬ë¨¸). ë©”ëª¨ë¦¬ ì œì•½ ì‹œ `en_core_web_sm`
* **ì„ë² ë”© ëª¨ë¸:** `"BAAI/bge-m3"` â†’ CPUâ€‘only í™˜ê²½ì—ì„  ë” ì‘ì€ ëª¨ë¸ ê³ ë ¤
* **LLM ë¼ë²¨ë§:** ë¡œì»¬ GGUF ëª¨ë¸ì„ ì“°ì§€ ì•Šìœ¼ë©´ `representation_model`ì—ì„œ LLM ë¶€ë¶„ ì œê±°
* **ì¶œë ¥ ê²½ë¡œ:** ëª¨ë“  ì‚°ì¶œë¬¼ì€ ê¸°ë³¸ì ìœ¼ë¡œ `./output/` í•˜ìœ„ì— ì €ì¥

---

##  ì¸ìš©

```plain
ì´ìš°ì„­. (2025â€‘04â€‘23). AI ê¸°ë°˜ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸í™”ë¥¼ í™œìš©í•œ ê±´ì¶• ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì´ ë¶„ì„
- ê±´ì¶•ë¬¼ ì´ë¯¸ì§€ ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„ ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ.
ëŒ€í•œê±´ì¶•í•™íšŒ í•™ìˆ ë°œí‘œëŒ€íšŒ ë…¼ë¬¸ì§‘, ì„œìš¸.
```

---

##  ê°ì‚¬ì˜ ë§

ë³¸ í”„ë¡œì íŠ¸ëŠ” **BERTopic**(UMAP/HDBSCAN + câ€‘TFâ€‘IDF, ë‹¤ì¤‘ í‘œí˜„, í’ë¶€í•œ ì‹œê°í™” ë“±)ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜/ê³ ê¸‰ ê¸°ëŠ¥ì€ BERTopic ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

---

##  ê¸°ì—¬

ë²„ê·¸ ì œë³´, ê¸°ëŠ¥ ì œì•ˆ, ì‹¤ì œ í™œìš© ì‚¬ë¡€ í™˜ì˜í•©ë‹ˆë‹¤. **Issue** ë˜ëŠ” **Pull Request**ë¥¼ ì—´ì–´ì£¼ì„¸ìš”.

---

##  ë¼ì´ì„ ìŠ¤

**MIT License**ë¡œ ë°°í¬ë©ë‹ˆë‹¤.

```
